<FrameworkSwitchCourse {fw} />

# ì²˜ìŒë¶€í„° ì¸ê³¼ ì–¸ì–´ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê¸°[[training-a-causal-language-model-from-scratch]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_tf.ipynb"},
]} />

{/if}

ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ëŠ” ì£¼ë¡œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³  ê·¸ë“¤ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ì‚¬ìš© ì‚¬ë¡€ì— ëŒ€í•´ ì„¸ì„¸í•œ ì¡°ì •ì„ í•´ì™”ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” [1ì¥](/course/chapter1)ì—ì„œ ë³¸ ê²ƒì²˜ëŸ¼ ì´ë¥¼ _ì „ì´ í•™ìŠµ_ì´ë¼ê³  ì¼ë°˜ì ìœ¼ë¡œ ë¶€ë¥´ë©°, í¬ì†Œí•œ ë¼ë²¨ì´ ìˆëŠ” ëŒ€ë¶€ë¶„ì˜ ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€ì— Transformer ëª¨ë¸ì„ ì ìš©í•˜ëŠ” ë§¤ìš° ì„±ê³µì ì¸ ì „ëµì…ë‹ˆë‹¤. ì´ë²ˆ ì¥ì—ì„œëŠ” ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ì„ ì·¨í•˜ì—¬ ì™„ì „íˆ ìƒˆë¡œìš´ ëª¨ë¸ì„ ì²˜ìŒë¶€í„° í›ˆë ¨ì‹œí‚¤ê² ìŠµë‹ˆë‹¤. ì´ê²ƒì€ ê·€í•˜ê°€ ë§ì€ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆê³  ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì— ì‚¬ìš©ëœ ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ì™€ ë§¤ìš° ë‹¤ë¥¸ ê²½ìš°ì— ì¢‹ì€ ì ‘ê·¼ ë°©ì‹ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ê²ƒì€ ì‚¬ì „ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸ì„ ë‹¨ìˆœíˆ ì„¸ì„¸íˆ ì¡°ì •í•˜ëŠ” ê²ƒë³´ë‹¤ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ì „ í›ˆë ¨í•˜ëŠ” ë° ìƒë‹¹íˆ ë” ë§ì€ ê³„ì‚° ë¦¬ì†ŒìŠ¤ë¥¼ í•„ìš”ë¡œ í•©ë‹ˆë‹¤. ìƒˆë¡œìš´ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¬ ìˆ˜ ìˆëŠ” ì˜ˆëŠ” ìŒì•… ë…¸íŠ¸ë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„° ì„¸íŠ¸, DNAì™€ ê°™ì€ ë¶„ì ì„œì—´, ë˜ëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ê°€ ìˆìŠµë‹ˆë‹¤. í›„ìëŠ” TabNineì™€ GitHubì˜ Copilotê³¼ ê°™ì€ ë„êµ¬ë“¤ ë•ë¶„ì— ìµœê·¼ì— ì¸ê¸°ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤. ì´ë“¤ì€ OpenAIì˜ Codex ëª¨ë¸ì—ì„œ íŒŒìƒëœ ê¸´ ì½”ë“œ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í…ìŠ¤íŠ¸ ìƒì„± ì‘ì—…ì€ GPT-2ì™€ ê°™ì€ ìê¸° íšŒê·€í˜• ë˜ëŠ” ì¸ê³¼ ì–¸ì–´ ëª¨ë¸ë¡œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¢‹ìŠµë‹ˆë‹¤.

ì´ ì„¹ì…˜ì—ì„œëŠ” ì½”ë“œ ìƒì„± ëª¨ë¸ì˜ ì¶•ì†ŒíŒì„ ë§Œë“¤ ê²ƒì…ë‹ˆë‹¤: ìš°ë¦¬ëŠ” Python ì½”ë“œì˜ ì¼ì¤„ ì™„ì„±ì— ì´ˆì ì„ ë§ì¶œ ê²ƒì´ë©°, ì „ì²´ í•¨ìˆ˜ë‚˜ í´ë˜ìŠ¤ ëŒ€ì‹  Python ì½”ë“œì˜ í•˜ìœ„ ì§‘í•©ì„ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. Python ë°ì´í„°ë¥¼ ë‹¤ë£° ë•Œ ì£¼ë¡œ `matplotlib`, `seaborn`, `pandas`, `scikit-learn` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì´ë£¨ì–´ì§„ Python ë°ì´í„° ê³¼í•™ ìŠ¤íƒê³¼ ìì£¼ ìƒí˜¸ ì‘ìš©í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•  ë•Œ íŠ¹ì • ëª…ë ¹ì„ ì°¾ì•„ì•¼ í•  ë•Œê°€ ë§ê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ í˜¸ì¶œì„ ìë™ìœ¼ë¡œ ì™„ì„±í•˜ëŠ” ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ë©´ ì¢‹ì„ ê²ƒì…ë‹ˆë‹¤.

<Youtube id="Vpjb1lu0MDk"/>

[6ì¥](/course/chapter6)ì—ì„œ ìš°ë¦¬ëŠ” Python ì†ŒìŠ¤ ì½”ë“œë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ íš¨ìœ¨ì ì¸ í† í¬ë‚˜ì´ì €ë¥¼ ìƒì„±í–ˆì§€ë§Œ, ì—¬ì „íˆ ëª¨ë¸ì„ ì‚¬ì „ í›ˆë ¨í•  ëŒ€ê·œëª¨ ë°ì´í„° ì„¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” GitHub ë¦¬í¬ì§€í† ë¦¬ì—ì„œ íŒŒìƒëœ Python ì½”ë“œ ì½”í¼ìŠ¤ì— ìš°ë¦¬ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ì ìš©í•  ê²ƒì…ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ `Trainer` APIì™€ ğŸ¤— ê°€ì†ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•  ê²ƒì…ë‹ˆë‹¤. ì‹œì‘í•´ë´…ì‹œë‹¤!

<iframe src="https://course-demos-codeparrot-ds.hf.space" frameBorder="0" height="300" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

ì´ê²ƒì€ ì‹¤ì œë¡œ ì´ ì„¹ì…˜ì—ì„œ ë³´ì—¬ì§„ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨í•˜ê³  Hubì— ì—…ë¡œë“œëœ ëª¨ë¸ì„ ì „ì‹œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ [here](https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28). í…ìŠ¤íŠ¸ ìƒì„±ì— ì•½ê°„ì˜ ë¬´ì‘ìœ„ì„±ì´ ë°œìƒí•˜ê¸° ë•Œë¬¸ì— ê²°ê³¼ê°€ ì•½ê°„ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
 
## ë°ì´í„° ìˆ˜ì§‘[[gathering-the-data]]

Python ì½”ë“œëŠ” GitHubì™€ ê°™ì€ ì½”ë“œ ë¦¬í¬ì§€í† ë¦¬ì—ì„œ í’ë¶€í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ìš°ë¦¬ëŠ” ëª¨ë“  Python ë¦¬í¬ì§€í† ë¦¬ë¥¼ ìŠ¤í¬ë©í•˜ì—¬ ë°ì´í„° ì„¸íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ [Transformers textbook](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/)ì—ì„œ ëŒ€ê·œëª¨ GPT-2 ëª¨ë¸ì„ ì‚¬ì „ í›ˆë ¨í•˜ê¸° ìœ„í•´ ì·¨í•œ ì ‘ê·¼ ë°©ì‹ì…ë‹ˆë‹¤. ì•½ 180GB í¬ê¸°ì˜ GitHub ë¤í”„ì¸ `codeparrot`ì´ë¼ëŠ” ì•½ 2000ë§Œ ê°œì˜ Python íŒŒì¼ì´ ë“¤ì–´ìˆëŠ” ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ìë“¤ì€ ë°ì´í„° ì„¸íŠ¸ë¥¼ êµ¬ì¶•í•œ í›„ ì´ë¥¼ [Hugging Face Hub](https://huggingface.co/datasets/transformersbook/codeparrot)ì— ê³µìœ í–ˆìŠµë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ ì „ì²´ ì½”í¼ìŠ¤ì—ì„œ í›ˆë ¨í•˜ëŠ” ê²ƒì€ ì‹œê°„ê³¼ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ê°€ ë§ì´ ì†Œëª¨ë˜ë¯€ë¡œ Python ë°ì´í„° ê³¼í•™ ìŠ¤íƒì— ê´€ë ¨ëœ ë°ì´í„° ì„¸íŠ¸ì˜ í•˜ìœ„ ì§‘í•©ë§Œ í•„ìš”í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ë¨¼ì € `codeparrot` ë°ì´í„° ì„¸íŠ¸ë¥¼ í•„í„°ë§í•˜ì—¬ ì´ ìŠ¤íƒì— ê´€ë ¨ëœ ëª¨ë“  íŒŒì¼ì„ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤. ë°ì´í„° ì„¸íŠ¸ì˜ í¬ê¸° ë•Œë¬¸ì— ë‹¤ìš´ë¡œë“œí•˜ëŠ” ê²ƒì„ í”¼í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì´ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ í•„í„°ë§í•  ê²ƒì…ë‹ˆë‹¤. ì•ì—ì„œ ì–¸ê¸‰í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì½”ë“œ ìƒ˜í”Œì„ í•„í„°ë§í•˜ëŠ” ë° ë„ì›€ì„ ì£¼ê¸° ìœ„í•´ ë‹¤ìŒ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤:

```py
def any_keyword_in_string(string, keywords):
    for keyword in keywords:
        if keyword in string:
            return True
    return False
```

ë‘ ê°€ì§€ ì˜ˆì œì—ì„œ ì´ê²ƒì„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]
example_1 = "import numpy as np"
example_2 = "import pandas as pd"

print(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)
```

```python out
False True
```

ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì„¸íŠ¸ë¥¼ ë§Œë“¤ê³  ìš°ë¦¬ê°€ ì›í•˜ëŠ” ìš”ì†Œë¥¼ í•„í„°ë§í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
from collections import defaultdict
from tqdm import tqdm
from datasets import Dataset


def filter_streaming_dataset(dataset, filters):
    filtered_dict = defaultdict(list)
    total = 0
    for sample in tqdm(iter(dataset)):
        total += 1
        if any_keyword_in_string(sample["content"], filters):
            for k, v in sample.items():
                filtered_dict[k].append(v)
    print(f"{len(filtered_dict['content'])/total:.2%} of data after filtering.")
    return Dataset.from_dict(filtered_dict)
```

ê·¸ëŸ° ë‹¤ìŒ ì´ í•¨ìˆ˜ë¥¼ ë‹¨ìˆœíˆ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì„¸íŠ¸ì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:


```py
# ì´ ì…€ì„ ì‹¤í–‰í•˜ëŠ” ë° ë§¤ìš° ì˜¤ëœ ì‹œê°„ì´ ê±¸ë¦¬ë¯€ë¡œ ì´ë¥¼ ê±´ë„ˆë›°ê³  ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°€ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤!
from datasets import load_dataset

split = "train"  # "valid"
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]

data = load_dataset(f"transformersbook/codeparrot-{split}", split=split, streaming=True)
filtered_data = filter_streaming_dataset(data, filters)
```

```python out
3.26% of data after filtering.
```
ì´ë ‡ê²Œ í•˜ë©´ ì›ë˜ ë°ì´í„° ì„¸íŠ¸ì˜ ì•½ 3%ë§Œ ë‚¨ê²Œ ë©ë‹ˆë‹¤. ì´ê²ƒì€ ì—¬ì „íˆ ìƒë‹¹íˆ í° ë°ì´í„° ì„¸íŠ¸ì…ë‹ˆë‹¤. ê²°ê³¼ ë°ì´í„° ì„¸íŠ¸ëŠ” 6GBì´ê³  60ë§Œ ê°œì˜ Python ìŠ¤í¬ë¦½íŠ¸ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤!

ì „ì²´ ë°ì´í„° ì„¸íŠ¸ë¥¼ í•„í„°ë§í•˜ëŠ” ë°ëŠ” ì‚¬ìš©í•˜ëŠ” ê¸°ê³„ ë° ëŒ€ì—­í­ì— ë”°ë¼ 2~3ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸´ ê³¼ì •ì„ ì§ì ‘ ìˆ˜í–‰í•˜ì§€ ì•Šìœ¼ë ¤ëŠ” ê²½ìš° í•„í„°ë§ëœ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆë„ë¡ Hubì—ì„œ ì œê³µí•©ë‹ˆë‹¤:

```py
from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="validation")

raw_datasets = DatasetDict(
    {
        "train": ds_train,  # .shuffle().select(range(50000)),
        "valid": ds_valid,  # .shuffle().select(range(500))
    }
)

raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 606720
    })
    valid: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 3322
    })
})
```

<Tip>

ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ì „ í›ˆë ¨í•˜ëŠ” ë° ì‹œê°„ì´ ê±¸ë¦´ ê²ƒì…ë‹ˆë‹¤. ë¨¼ì € ìœ„ì˜ ë‘ ë¶€ë¶„ ë¼ì¸ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ë°ì´í„°ì˜ ìƒ˜í”Œì—ì„œ í›ˆë ¨ ë£¨í”„ë¥¼ ì‹¤í–‰í•˜ê³  í›ˆë ¨ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ê³  ëª¨ë¸ì´ ì €ì¥ë˜ëŠ”ì§€ í™•ì¸í•˜ì‹­ì‹œì˜¤. í›ˆë ¨ ë£¨í”„ì˜ ëì— í´ë”ë¥¼ ë§Œë“¤ê±°ë‚˜ ì˜¤íƒ€ê°€ ìˆì–´ì„œ ë§ˆì§€ë§‰ ë‹¨ê³„ì—ì„œ í›ˆë ¨ ì‹¤í–‰ì´ ì‹¤íŒ¨í•˜ëŠ” ê²ƒë³´ë‹¤ ë” ì¢‹ìŠµë‹ˆë‹¤!

</Tip>

ë°ì´í„° ì„¸íŠ¸ì˜ ì˜ˆì œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ê° í•„ë“œì˜ ì²˜ìŒ 200ìë§Œ í‘œì‹œí•˜ê² ìŠµë‹ˆë‹¤:

```py
for key in raw_datasets["train"][0]:
    print(f"{key.upper()}: {raw_datasets['train'][0][key][:200]}")
```

```python out
'REPO_NAME: kmike/scikit-learn'
'PATH: sklearn/utils/__init__.py'
'COPIES: 3'
'SIZE: 10094'
'''CONTENT: """
The :mod:`sklearn.utils` module includes various utilites.
"""

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause'''
```

`content` í•„ë“œê°€ ìš°ë¦¬ì˜ ëª¨ë¸ì´ í›ˆë ¨í•  ì½”ë“œë¥¼ í¬í•¨í•˜ê³  ìˆìŒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì œ ë°ì´í„° ì„¸íŠ¸ê°€ ìˆìœ¼ë¯€ë¡œ ì‚¬ì „ í›ˆë ¨ì— ì í•©í•œ í˜•ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¤€ë¹„í•´ì•¼ í•©ë‹ˆë‹¤.

## ë°ì´í„° ì„¸íŠ¸ ì¤€ë¹„í•˜ê¸°[[preparing-the-dataset]]

<Youtube id="ma1TrR7gE7I"/>

ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” ë°ì´í„°ë¥¼ í† í°í™”í•˜ì—¬ í›ˆë ¨ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì£¼ë¡œ ì§§ì€ í•¨ìˆ˜ í˜¸ì¶œì„ ìë™ìœ¼ë¡œ ì™„ì„±í•˜ê¸° ìœ„í•œ ê²ƒì´ë¯€ë¡œ ì»¨í…ìŠ¤íŠ¸ í¬ê¸°ë¥¼ ìƒëŒ€ì ìœ¼ë¡œ ì‘ê²Œ ìœ ì§€í•  ê²ƒì…ë‹ˆë‹¤. ì´ê²ƒì€ ëª¨ë¸ì„ í›¨ì”¬ ë” ë¹ ë¥´ê²Œ í›ˆë ¨í•  ìˆ˜ ìˆê²Œ í•˜ê³  ë©”ëª¨ë¦¬ë¥¼ í›¨ì”¬ ì ê²Œ í•„ìš”ë¡œ í•©ë‹ˆë‹¤. ì–´í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°–ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤ë©´(ì˜ˆë¥¼ ë“¤ì–´, íŒŒì¼ì˜ í•¨ìˆ˜ ì •ì˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì´ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ë„ë¡ í•˜ë ¤ë©´), í•´ë‹¹ ìˆ«ìë¥¼ ì¦ê°€ì‹œí‚¤ë˜ ì´ëŠ” ë” ë§ì€ GPU ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ì—¼ë‘ì— ë‘ì„¸ìš”. ì§€ê¸ˆì€ ì»¨í…ìŠ¤íŠ¸ í¬ê¸°ë¥¼ 128 í† í°ìœ¼ë¡œ ê³ ì •í•˜ê² ìŠµë‹ˆë‹¤. ì´ëŠ” GPT-2ë‚˜ GPT-3ì—ì„œ ì‚¬ìš©ë˜ëŠ” 1,024 ë˜ëŠ” 2,048ê³¼ëŠ” ë‹¬ë¦¬ ìƒëŒ€ì ìœ¼ë¡œ ì‘ìŠµë‹ˆë‹¤.

ëŒ€ë¶€ë¶„ì˜ ë¬¸ì„œëŠ” 128ê°œ ì´ìƒì˜ í† í°ì„ í¬í•¨í•˜ë¯€ë¡œ ìµœëŒ€ ê¸¸ì´ë¡œ ì…ë ¥ì„ ìë¥´ë©´ ë°ì´í„° ì„¸íŠ¸ì˜ ë§ì€ ë¶€ë¶„ì´ ì‚¬ë¼ì§‘ë‹ˆë‹¤. ëŒ€ì‹  `return_overflowing_tokens` ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ì…ë ¥ì„ í† í°í™”í•˜ê³  ì—¬ëŸ¬ ì²­í¬ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” [6ì¥](/course/chapter6/4)ì—ì„œ í–ˆë˜ ê²ƒì²˜ëŸ¼ ì´ê²ƒì„ í•  ê²ƒì…ë‹ˆë‹¤. ë˜í•œ `return_length` ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ìƒì„±ëœ ê° ì²­í¬ì˜ ê¸¸ì´ë¥¼ ë°˜í™˜í•  ê²ƒì…ë‹ˆë‹¤. ë§ˆì§€ë§‰ ì²­í¬ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ í¬ê¸°ë³´ë‹¤ ì‘ì„ ê²ƒì´ë¯€ë¡œ íŒ¨ë”© ë¬¸ì œë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì´ëŸ¬í•œ ì¡°ê°ë“¤ì„ ì œê±°í•©ë‹ˆë‹¤. ìš°ë¦¬ê°€ ì´ë¯¸ ì¶©ë¶„í•œ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ ì¡°ê°ë“¤ì€ ì‹¤ì œë¡œ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg" alt="Chunking a large texts in several pieces."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts-dark.svg" alt="Chunking a large texts in several pieces."/>
</div>

ì´ ì‘ì—…ì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ë¥¼ ì •í™•íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ì²˜ìŒ ë‘ ì˜ˆì œë¥¼ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```py
from transformers import AutoTokenizer

context_length = 128
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
    raw_datasets["train"][:2]["content"],
    truncation=True,
    max_length=context_length,
    return_overflowing_tokens=True,
    return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")
```

```python out
Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

ì´ë ‡ê²Œ í•˜ë©´ ìš°ë¦¬ëŠ” ë‘ ì˜ˆì œì—ì„œ ì´ 34ê°œì˜ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì–»ê²Œ ë©ë‹ˆë‹¤. ì²­í¬ì˜ ê¸¸ì´ë¥¼ ì‚´í´ë³´ë©´, ë‘ ë¬¸ì„œ ëª¨ë‘ ëì— ìˆëŠ” ì²­í¬ê°€ 128í† í° ë¯¸ë§Œì„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤(ê°ê° 117ê³¼ 41). ì´ë“¤ì€ ìš°ë¦¬ê°€ ê°€ì§„ ì´ ì²­í¬ ì¤‘ì— ì‘ì€ ì¼ë¶€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ë¥¼ ì•ˆì „í•˜ê²Œ ë²„ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. `overflow_to_sample_mapping` í•„ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì–´ë–¤ ì²­í¬ê°€ ì–´ë–¤ ì…ë ¥ ìƒ˜í”Œì— ì†í•˜ëŠ”ì§€ë„ ì¬êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë°ì—ëŠ” ğŸ¤— ë°ì´í„°ì…‹ì˜ `Dataset.map()` í•¨ìˆ˜ì˜ ìœ ìš©í•œ ê¸°ëŠ¥ì„ í™œìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì¼ëŒ€ì¼ ë§¤í•‘ì„ ìš”êµ¬í•˜ì§€ ì•Šìœ¼ë©°, í•˜ë‚˜ì˜ ì…ë ¥ ë°°ì¹˜ë³´ë‹¤ ë” ë§ê±°ë‚˜ ë” ì ì€ ìš”ì†Œë¥¼ í¬í•¨í•˜ëŠ” ë°°ì¹˜ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°ì´í„° ì¦ê°• ë˜ëŠ” ë°ì´í„° í•„í„°ë§ê³¼ ê°™ì€ ì‘ì—…ì„ ìˆ˜í–‰í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²½ìš°ì—ëŠ” ê° ìš”ì†Œë¥¼ ì§€ì •ëœ ë¬¸ë§¥ í¬ê¸°ì˜ ì²­í¬ë¡œ í† í°í™”í•  ë•Œë§ˆë‹¤ ê° ë¬¸ì„œì—ì„œ ë§ì€ ìƒ˜í”Œì„ ìƒì„±í•©ë‹ˆë‹¤. ë‹¤ë§Œ, ê¸°ì¡´ ì—´ì„ ì‚­ì œí•´ì•¼ í•˜ë¯€ë¡œ í¬ê¸°ê°€ ì¶©ëŒí•˜ëŠ” ê²ƒì— ìœ ì˜í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ ì§€í•˜ë ¤ë©´ í•´ë‹¹ ì—´ì„ ì ì ˆíˆ ë°˜ë³µí•˜ê³  `Dataset.map()` í˜¸ì¶œ ë‚´ì—ì„œ ë°˜í™˜í•˜ë©´ ë©ë‹ˆë‹¤.

```py
def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 16702061
    })
    valid: Dataset({
        features: ['input_ids'],
        num_rows: 93164
    })
})
```

ì´ì œ ê°ê° 128 í† í°ì”© 1,670ë§Œ ê°œì˜ ì˜ˆì œê°€ ìˆìœ¼ë©°, ì´ 21ì–µ í† í°ì— í•´ë‹¹í•©ë‹ˆë‹¤. ì°¸ê³ ë¡œ OpenAIì˜ GPT-3ì™€ Codex ëª¨ë¸ì€ ê°ê° 300ì–µê³¼ 1000ì–µ í† í°ì— ëŒ€í•´ í›ˆë ¨ë˜ì—ˆìœ¼ë©°, Codex ëª¨ë¸ì€ GPT-3 ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ì„¹ì…˜ì˜ ëª©í‘œëŠ” ì´ëŸ¬í•œ ëª¨ë¸ê³¼ ê²½ìŸí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë°ì´í„° ê³¼í•™ìë¥¼ ìœ„í•œ ë¹ ë¥¸ ìë™ ì™„ì„± ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ì¶•ì†ŒíŒì„ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.

ë°ì´í„° ì„¸íŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆìœ¼ë¯€ë¡œ ì´ì œ ëª¨ë¸ì„ ì„¤ì •í•´ ë´…ì‹œë‹¤!

<Tip>

âœï¸ **í•´ë³´ì„¸ìš”!** ì—¬ê¸°ì„œëŠ” ì‘ì€ ì»¨í…ìŠ¤íŠ¸ ì°½ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì»¨í…ìŠ¤íŠ¸ í¬ê¸°ë³´ë‹¤ ì‘ì€ ëª¨ë“  ì²­í¬ë¥¼ ì œê±°í•˜ëŠ” ê²ƒì´ í° ë¬¸ì œê°€ ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ í¬ê¸°ë¥¼ ëŠ˜ë¦¬ë©´(ë˜ëŠ” ì§§ì€ ë¬¸ì„œì˜ ë§ë­‰ì¹˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°) ë²„ë ¤ì§„ ì²­í¬ì˜ ë¹„ìœ¨ë„ ëŠ˜ì–´ë‚©ë‹ˆë‹¤. ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” ë” íš¨ìœ¨ì ì¸ ë°©ë²•ì€ ëª¨ë“  í† í°í™”ëœ ìƒ˜í”Œì„ `eos_token_id` í† í° ì‚¬ì´ì— ê²°í•©í•œ ë‹¤ìŒ ì—°ê²°ëœ ì‹œí€€ìŠ¤ì—ì„œ ì²­í¬ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì—°ìŠµìœ¼ë¡œ `tokenize()` í•¨ìˆ˜ë¥¼ ìˆ˜ì •í•˜ì—¬ í•´ë‹¹ ë°©ë²•ì„ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤. í† í°í™”ì—ì„œ `truncation=False`ë¥¼ ì„¤ì •í•˜ê³  ë‹¤ë¥¸ ì¸ìˆ˜ë¥¼ í† í¬ë‚˜ì´ì €ì—ì„œ ì œê±°í•´ì•¼ ì „ì²´ í† í° ID ì‹œí€€ìŠ¤ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

</Tip>


## ìƒˆ ëª¨ë¸ ì´ˆê¸°í™”[[initializing-a-new-model]]

ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” ìƒˆë¡œìš´ GPT-2 ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ëª¨ë¸ì˜ í¬ê¸°ë¥¼ GPT-2 ì‘ì€ ëª¨ë¸ê³¼ ê°™ê²Œ í•˜ê¸° ìœ„í•´ ìš°ë¦¬ ëª¨ë¸ì— ëŒ€í•œ ë™ì¼í•œ êµ¬ì„±ì„ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì‚¬ì „ í›ˆë ¨ëœ êµ¬ì„±ì„ ë¡œë“œí•˜ê³  í† í¬ë‚˜ì´ì € í¬ê¸°ê°€ ëª¨ë¸ ì–´íœ˜ í¬ê¸°ì™€ ì¼ì¹˜í•˜ë„ë¡í•˜ê³  `bos`ì™€ `eos`(ì‹œí€€ìŠ¤ì˜ ì‹œì‘ê³¼ ë) í† í° IDë¥¼ ì „ë‹¬í•  ê²ƒì…ë‹ˆë‹¤:

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

ì´ êµ¬ì„±ì„ ì‚¬ìš©í•˜ì—¬ ìƒˆ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ ìš°ë¦¬ê°€ ì‹¤ì œë¡œ ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê¸° ë•Œë¬¸ì— `from_pretrained()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì²« ë²ˆì§¸ ê²½ìš°ì…ë‹ˆë‹¤:

```py
model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")
```

```python out
GPT-2 size: 124.2M parameters
```

{:else}

```py
from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

ê·¸ëŸ° ë‹¤ìŒì´ êµ¬ì„±ì„ ì‚¬ìš©í•˜ì—¬ ìƒˆ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ ì‹¤ì œë¡œ ìš°ë¦¬ê°€ ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê¸° ë•Œë¬¸ì— ìš°ë¦¬ê°€ `from_pretrained()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì²« ë²ˆì§¸ ê²½ìš°ì…ë‹ˆë‹¤:

```py
model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  # Builds the model
model.summary()
```

```python out
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
transformer (TFGPT2MainLayer multiple                  124242432 
=================================================================
Total params: 124,242,432
Trainable params: 124,242,432
Non-trainable params: 0
_________________________________________________________________
```

{/if}

ìš°ë¦¬ ëª¨ë¸ì€ ì¡°ì •í•´ì•¼ í•  124Mê°œì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°–ê³  ìˆìŠµë‹ˆë‹¤. í›ˆë ¨ì„ ì‹œì‘í•˜ê¸° ì „ì— ë°°ì¹˜ë¥¼ ìƒì„±í•˜ëŠ” ë°ì´í„° ì½œë ˆì´í„°ë¥¼ ì„¤ì •í•´ì•¼í•©ë‹ˆë‹¤. íŠ¹íˆ ì–¸ì–´ ëª¨ë¸ë§ì— íŠ¹í™”ëœ `DataCollatorForLanguageModeling` ì½œë ˆì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì´ë¦„ì—ì„œ ì•½ê°„ ì•”ì‹œë©ë‹ˆë‹¤). ë°°ì¹˜ë¥¼ ìŒ“ê³  íŒ¨ë”©í•˜ëŠ” ê²ƒ ì™¸ì—ë„ ì´ ì½œë ˆì´í„°ëŠ” ì–¸ì–´ ëª¨ë¸ ë ˆì´ë¸”ì„ ìƒì„±í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì¸ê³¼ ì–¸ì–´ ëª¨ë¸ë§ì—ì„œ ì…ë ¥ì€ ë ˆì´ë¸” ì—­í• ì„ í•˜ê¸°ë„ í•©ë‹ˆë‹¤(í•˜ë‚˜ì˜ ìš”ì†Œë§Œ ì‹œí”„íŠ¸ëœ ê²ƒ). ì´ ë°ì´í„° ì½œë ˆì´í„°ëŠ” í›ˆë ¨ ì¤‘ì— ë ˆì´ë¸”ì„ ìƒì„±í•˜ì—¬ `input_ids`ë¥¼ ì¤‘ë³µí•˜ì—¬ ìƒì„±í•  í•„ìš”ê°€ ì—†ë„ë¡í•©ë‹ˆë‹¤.

`DataCollatorForLanguageModeling`ì€ ë§ˆìŠ¤í¬ ì–¸ì–´ ëª¨ë¸ë§(MLM) ë° ì¸ê³¼ ì–¸ì–´ ëª¨ë¸ë§(CLM)ì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ ì´ ì½œë ˆì´í„°ëŠ” MLMì„ ìœ„í•´ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ì§€ë§Œ `mlm=False` ì¸ìˆ˜ë¥¼ ì„¤ì •í•˜ì—¬ CLMìœ¼ë¡œ ì „í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

{#if fw === 'pt'}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
```

{:else}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")
```

{/if}

ì˜ˆì œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:

```py
out = data_collator([tokenized_datasets["train"][i] for i in range(5)])
for key in out:
    print(f"{key} shape: {out[key].shape}")
```

{#if fw === 'pt'}

```python out
input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])
```

{:else}

```python out
input_ids shape: (5, 128)
attention_mask shape: (5, 128)
labels shape: (5, 128)
```

{/if}

ì˜ˆì œê°€ ìŒ“ì—¬ ìˆê³  ëª¨ë“  í…ì„œê°€ ë™ì¼í•œ ëª¨ì–‘ì„ ê°€ì§€ê³  ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

{#if fw === 'tf'}

ì´ì œ `prepare_tf_dataset()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì„¸íŠ¸ë¥¼ TensorFlow ë°ì´í„° ì„¸íŠ¸ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ„ì—ì„œ ìƒì„±í•œ ë°ì´í„° ì½œë ˆì´í„°ì™€ í•¨ê»˜:

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_dataset["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_dataset["valid"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)
```

{/if}

<Tip warning={true}>

âš ï¸ ì…ë ¥ê³¼ ë ˆì´ë¸”ì„ ì •ë ¬í•˜ì—¬ ë§ì¶”ëŠ” ê²ƒì€ ëª¨ë¸ ë‚´ë¶€ì—ì„œ ë°œìƒí•˜ë¯€ë¡œ ë°ì´í„° ì½œë ˆì´í„°ëŠ” ë ˆì´ë¸”ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ì…ë ¥ì„ ë‹¨ìˆœíˆ ë³µì‚¬í•©ë‹ˆë‹¤.

</Tip>


ì´ì œ ìš°ë¦¬ëŠ” ëª¨ë¸ì„ ì‹¤ì œë¡œ í›ˆë ¨í•  ëª¨ë“  ì¤€ë¹„ë¥¼ ë§ˆì³¤ìŠµë‹ˆë‹¤. ì´ì œ í›ˆë ¨ì„ ì‹œì‘í•˜ê¸° ì „ì— Hugging Faceì— ë¡œê·¸ì¸í•´ì•¼ í•©ë‹ˆë‹¤. ë…¸íŠ¸ë¶ì—ì„œ ì‘ì—… ì¤‘ì´ë¼ë©´ ë‹¤ìŒ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œê·¸ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
from huggingface_hub import notebook_login

notebook_login()
```

ì´ê²ƒì€ Hugging Face ë¡œê·¸ì¸ ìê²© ì¦ëª…ì„ ì…ë ¥í•  ìˆ˜ ìˆëŠ” ìœ„ì ¯ì„ í‘œì‹œí•©ë‹ˆë‹¤.

ë…¸íŠ¸ë¶ì—ì„œ ì‘ì—…í•˜ì§€ ì•ŠëŠ” ê²½ìš° í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ì¤„ì„ ì…ë ¥í•˜ë©´ë©ë‹ˆë‹¤:

```bash
huggingface-cli login
```

{#if fw === 'pt'}

ë‚˜ë¨¸ì§€ ì‘ì—…ì€ í›ˆë ¨ ì¸ìˆ˜ë¥¼ êµ¬ì„±í•˜ê³  `Trainer`ë¥¼ ì‹¤í–‰í•˜ëŠ” ê²ƒë¿ì…ë‹ˆë‹¤. ì½”ì‚¬ì¸ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ê³¼ ì¼ë¶€ ì›œì—…, ê·¸ë¦¬ê³  íš¨ê³¼ì ì¸ ë°°ì¹˜ í¬ê¸°ì¸ 256(`per_device_train_batch_size` * `gradient_accumulation_steps`)ë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. ê·¸ë¼ë””ì–¸íŠ¸ ëˆ„ì ì€ ë‹¨ì¼ ë°°ì¹˜ê°€ ë©”ëª¨ë¦¬ì— ë§ì§€ ì•Šì„ ë•Œ ì‚¬ìš©ë˜ë©° ì—¬ëŸ¬ ì „ë°©í–¥/í›„ë°©í–¥ íŒ¨ìŠ¤ë¥¼ í†µí•´ ì ì§„ì ìœ¼ë¡œ ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ ëˆ„ì í•©ë‹ˆë‹¤. ì´ê²ƒì„ ğŸ¤— Accelerateì™€ í•¨ê»˜ í›ˆë ¨ ë£¨í”„ë¥¼ ë§Œë“¤ ë•Œ ë³¼ ê²ƒì…ë‹ˆë‹¤.

```py
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="codeparrot-ds",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)
```

ì´ì œ `Trainer`ë¥¼ ì‹œì‘í•˜ê³  í›ˆë ¨ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ë©´ ë©ë‹ˆë‹¤. í›ˆë ¨ ì„¸íŠ¸ì˜ ì „ì²´ ë˜ëŠ” ì¼ë¶€ì— ì‹¤í–‰í•˜ëŠ”ì§€ì— ë”°ë¼ ì‹œê°„ì´ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•´ ì‹¤í–‰í•˜ëŠ” ê²½ìš° 20ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë©°, ì¼ë¶€ ë°ì´í„°ì…‹ì— ëŒ€í•´ ì‹¤í–‰í•˜ëŠ” ê²½ìš° 2ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì—¬ìœ  ìˆê²Œ ëª‡ ì”ì˜ ì»¤í”¼ë¥¼ ë§ˆì‹œê³  ì¢‹ì€ ì±…ì„ ì½ìœ¼ë©´ ë©ë‹ˆë‹¤!

```py
trainer.train()
```

í›ˆë ¨ì´ ì™„ë£Œë˜ë©´ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ Hubì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
trainer.push_to_hub()
```

{:else}

ë‚˜ë¨¸ì§€ í•  ì¼ì€ í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ êµ¬ì„±í•˜ê³  `compile()` ë° `fit()`ì„ í˜¸ì¶œí•˜ëŠ” ê²ƒë¿ì…ë‹ˆë‹¤. í›ˆë ¨ì˜ ì•ˆì •ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ì¼ì •ì„ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.

```py
from transformers import create_optimizer
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

ì´ì œ `model.fit()`ì„ í˜¸ì¶œí•˜ê³  í›ˆë ¨ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ë©´ ë©ë‹ˆë‹¤. ì „ì²´ ë°ì´í„°ì…‹ ë˜ëŠ” ì¼ë¶€ ë°ì´í„°ì…‹ì—ì„œ ì‹¤í–‰í•˜ëŠëƒì— ë”°ë¼ í›ˆë ¨ì— ê±¸ë¦¬ëŠ” ì‹œê°„ì´ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì „ì²´ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” ê²½ìš° 20ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë‹ˆ, ì¢‹ì€ ì±…ê³¼ ì»¤í”¼ë¥¼ ì¤€ë¹„í•˜ì„¸ìš”! í›ˆë ¨ì´ ì™„ë£Œë˜ë©´ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ Hubì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="codeparrot-ds", tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```
<Tip>

âœï¸ **í•´ë³´ì„¸ìš”!** `TrainingArguments`ì— ëª‡ ì¤„ì˜ ì½”ë“œë§Œ ì¶”ê°€í•˜ë©´ ì›ì‹œ í…ìŠ¤íŠ¸ì—ì„œ GPT-2ë¥¼ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‹œë„í•´ë³´ê³  ì–¼ë§ˆë‚˜ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”!

</Tip>

<Tip>

{#if fw === 'pt'}

ğŸ’¡ ì—¬ëŸ¬ë¶„ì˜ ë¨¸ì‹ ì— ë‹¤ì¤‘ GPUê°€ ìˆëŠ” ê²½ìš° ì½”ë“œë¥¼ ì‹¤í–‰í•´ë³´ì„¸ìš”. `Trainer`ëŠ” ìë™ìœ¼ë¡œ ì—¬ëŸ¬ ë¨¸ì‹ ì„ ê´€ë¦¬í•˜ë©°, ì´ë ‡ê²Œ í•˜ë©´ í•™ìŠµ ì†ë„ê°€ í¬ê²Œ í–¥ìƒë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

{:else}

ğŸ’¡ ì—¬ëŸ¬ë¶„ì˜ ë¨¸ì‹ ì— ë‹¤ì¤‘ GPUê°€ ìˆëŠ” ê²½ìš° `MirroredStrategy` ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ ìƒë‹¹íˆ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. `tf.distribute.MirroredStrategy` ê°ì²´ë¥¼ ë§Œë“¤ê³ , `to_tf_dataset()` ë˜ëŠ” `prepare_tf_dataset()` ë©”ì„œë“œ ë° ëª¨ë¸ ìƒì„± ë° `fit()` í˜¸ì¶œì´ ëª¨ë‘ í•´ë‹¹ `scope()` ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ì‹¤í–‰ë˜ë„ë¡ í•´ì•¼ í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ì—¬ê¸°](https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

{/if}

</Tip>

## íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•œ ì½”ë“œ ìƒì„±[[code-generation-with-a-pipeline]]

ì´ì œê°€ ëª¨ë¸ì˜ ì‹¤ì œ ì‘ë™ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ëŠ” ìˆœê°„ì…ë‹ˆë‹¤! ë¡œê·¸ì—ì„œ ì†ì‹¤ì´ ê¾¸ì¤€íˆ ê°ì†Œí–ˆìŒì„ ë³¼ ìˆ˜ ìˆì§€ë§Œ, ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´ ëª‡ ê°€ì§€ í”„ë¡¬í”„íŠ¸ì—ì„œ ì–¼ë§ˆë‚˜ ì˜ ì‘ë™í•˜ëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ëª¨ë¸ì„ í…ìŠ¤íŠ¸ ìƒì„± `íŒŒì´í”„ë¼ì¸`ìœ¼ë¡œ ë˜í•‘í•˜ê³ , GPUê°€ ìˆëŠ” ê²½ìš° ë¹ ë¥¸ ìƒì„±ì„ ìœ„í•´ GPUì— ë„£ê² ìŠµë‹ˆë‹¤:

{#if fw === 'pt'}

```py
import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
    "text-generation", model="huggingface-course/codeparrot-ds", device=device
)
```

{:else}

```py
from transformers import pipeline

course_model = TFGPT2LMHeadModel.from_pretrained("huggingface-course/codeparrot-ds")
course_tokenizer = AutoTokenizer.from_pretrained("huggingface-course/codeparrot-ds")
pipe = pipeline(
    "text-generation", model=course_model, tokenizer=course_tokenizer, device=0
)
```

{/if}

ê°„ë‹¨í•œ ì‚°ì ë„ë¥¼ ë§Œë“œëŠ” ê°„ë‹¨í•œ ì‘ì—…ìœ¼ë¡œ ì‹œì‘í•´ë³´ê² ìŠµë‹ˆë‹¤:

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
plt.scatter(x, y)

# create scatter
```

ê²°ê³¼ê°€ ì˜¬ë°”ë¥´ê²Œ ë³´ì…ë‹ˆë‹¤. `pandas` ì‘ì—…ì—ë„ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤. ë‘ ë°°ì—´ì—ì„œ `DataFrame`ì„ ë§Œë“¤ ìˆ˜ ìˆëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for
```

ì¢‹ìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ë‹µë³€ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ì—´ `x`ë¥¼ ë‹¤ì‹œ ì‚½ì…í•©ë‹ˆë‹¤. ìƒì„±ëœ í† í° ìˆ˜ê°€ ì œí•œë˜ì–´ ìˆìœ¼ë¯€ë¡œ ë‹¤ìŒ `for` ë£¨í”„ê°€ ì˜ë ¤ ë‚˜ê°‘ë‹ˆë‹¤. ë” ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³  ëª¨ë¸ì´ `groupby` ì‘ì—…ì„ ë„ì™€ì£¼ëŠ”ì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤:

```py
txt = """\
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
profession = df.groupby(['profession']).mean()

# compute the
```

ê½¤ ê´œì°®ìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì˜¬ë°”ë¥¸ ë°©ë²•ì…ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ `scikit-learn`ì„ ì‚¬ìš©í•˜ì—¬ ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì„ ì„¤ì •í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤:

```py
txt = """
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf
```

{#if fw === 'tf'}

ì´ëŸ¬í•œ ëª‡ ê°€ì§€ ì˜ˆë¥¼ ì‚´í´ë³´ë©´ ëª¨ë¸ì´ Python ë°ì´í„° ê³¼í•™ ìŠ¤íƒì˜ ì¼ë¶€ êµ¬ë¬¸ì„ í•™ìŠµí•œ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ë¬¼ë¡  ì‹¤ì œ ì„¸ê³„ì— ë°°í¬í•˜ê¸° ì „ì— ëª¨ë¸ì„ ë” ì² ì €í•˜ê²Œ í‰ê°€í•´ì•¼ í•˜ì§€ë§Œ, ì´ëŠ” ì—¬ì „íˆ ì¸ìƒì ì¸ í”„ë¡œí† íƒ€ì…ì…ë‹ˆë‹¤.

{:else}

ì´ëŸ¬í•œ ëª‡ ê°€ì§€ ì˜ˆë¥¼ ì‚´í´ë³´ë©´ ëª¨ë¸ì´ Python ë°ì´í„° ê³¼í•™ ìŠ¤íƒì˜ ì¼ë¶€ êµ¬ë¬¸ì„ í•™ìŠµí•œ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤(ë¬¼ë¡ , ì‹¤ì œ ì„¸ê³„ì—ì„œ ëª¨ë¸ì„ ë°°í¬í•˜ê¸° ì „ì— ë” ì² ì €íˆ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤). ë•Œë¡œëŠ” ì£¼ì–´ì§„ ì‚¬ìš© ì‚¬ë¡€ì— í•„ìš”í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ëª¨ë¸ í•™ìŠµì„ ë” ë§ì´ ì‚¬ìš©ì ì •ì˜í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë°°ì¹˜ í¬ê¸°ë¥¼ ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ê±°ë‚˜ ì¡°ê±´ë¶€ í•™ìŠµ ë£¨í”„ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‚˜ìœ ì˜ˆì œë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê±´ë„ˆë›°ëŠ” ê²½ìš° ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”? `Trainer`ë¥¼ ì„œë¸Œ í´ë˜ìŠ¤í™”í•˜ê³  í•„ìš”í•œ ë³€ê²½ ì‚¬í•­ì„ ì¶”ê°€í•˜ëŠ” ê²ƒì´ í•œ ê°€ì§€ ì˜µì…˜ì¼ ìˆ˜ ìˆì§€ë§Œ, ë•Œë¡œëŠ” í•™ìŠµ ë£¨í”„ë¥¼ ì²˜ìŒë¶€í„° ì‘ì„±í•˜ëŠ” ê²ƒì´ ë” ê°„ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ğŸ¤— Accelerateê°€ í•„ìš”í•©ë‹ˆë‹¤.

{/if}

{#if fw === 'pt'}

## ğŸ¤— Accelerateë¡œ í•™ìŠµ[[training-with-accelerate]]

`Trainer`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì•½ê°„ì˜ ì‚¬ìš©ì ì •ì˜ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë•Œë¡œëŠ” í•™ìŠµ ë£¨í”„ë¥¼ ì™„ì „íˆ ì œì–´í•˜ê±°ë‚˜ ì–´ë–¤ ë…íŠ¹í•œ ë³€ê²½ ì‚¬í•­ì„ ë§Œë“¤ê³  ì‹¶ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²½ìš° ğŸ¤— Accelerateê°€ ì¢‹ì€ ì„ íƒì´ë©°, ì´ ì„¹ì…˜ì—ì„œëŠ” ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë‹¨ê³„ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ì¢€ ë” í¥ë¯¸ë¡œìš´ ì ì€ í•™ìŠµ ë£¨í”„ì— ì¶”ê°€ë¡œ ë³€í˜•ì„ ì¶”ê°€í•  ê²ƒì…ë‹ˆë‹¤.

<Youtube id="Hm8_PgVTFuc"/>

ì£¼ë¡œ ë°ì´í„° ê³¼í•™ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ëŒ€í•œ í•©ë¦¬ì ì¸ ìë™ ì™„ì„±ì„ ì›í•˜ê¸° ë•Œë¬¸ì—, ì´ëŸ¬í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë” ë§ì´ í™œìš©í•˜ëŠ” í•™ìŠµ ìƒ˜í”Œì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ê²ƒì´ í•©ë¦¬ì ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” `plt`, `pd`, `sk`, `fit` ë° `predict`ì™€ ê°™ì€ í‚¤ì›Œë“œë¥¼ í†µí•´ ì´ëŸ¬í•œ ì˜ˆì œë¥¼ ì‰½ê²Œ ì‹ë³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒë“¤ì€ ê°ê° `matplotlib.pyplot`, `pandas`, `sklearn`ì˜ ê°€ì¥ ë¹ˆë²ˆí•œ import ì´ë¦„ ë° í›„ìì˜ fit/predict íŒ¨í„´ì…ë‹ˆë‹¤. ì´ë“¤ì„ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ í‘œí˜„í•œë‹¤ë©´ ì…ë ¥ ì‹œí€€ìŠ¤ì— ë‚˜íƒ€ë‚˜ëŠ”ì§€ ì‰½ê²Œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í† í°ì—ëŠ” ê³µë°± ì ‘ë‘ì‚¬ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í† í¬ë‚˜ì´ì € ì–´íœ˜ì—ì„œ í•´ë‹¹ ë²„ì „ë„ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. ì´ê²ƒì´ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ í† í°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤. ì´ ì¤‘ í•˜ë‚˜ëŠ” ì—¬ëŸ¬ í† í°ìœ¼ë¡œ ë¶„í• ë˜ì–´ì•¼ í•©ë‹ˆë‹¤:

```py
keytoken_ids = []
for keyword in [
    "plt",
    "pd",
    "sk",
    "fit",
    "predict",
    " plt",
    " pd",
    " sk",
    " fit",
    " predict",
    "testtest",
]:
    ids = tokenizer([keyword]).input_ids[0]
    if len(ids) == 1:
        keytoken_ids.append(ids[0])
    else:
        print(f"Keyword has not single token: {keyword}")
```

```python out
'Keyword has not single token: testtest'
```

ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤! ì´ì œ ì…ë ¥ ì‹œí€€ìŠ¤, ë¡œì§“ ë° ë°©ê¸ˆ ì„ íƒí•œ í‚¤ í† í°ì„ ì‚¬ìš©í•˜ëŠ” ì‚¬ìš©ì ì •ì˜ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € ë¡œì§“ ë° ì…ë ¥ì„ ì •ë ¬í•´ì•¼ í•©ë‹ˆë‹¤. ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì˜¤ë¥¸ìª½ìœ¼ë¡œ í•œ ì¹¸ ì´ë™í•˜ë©´ ë¼ë²¨ì´ í˜•ì„±ë˜ë¯€ë¡œ, ë¼ë²¨ì€ í˜„ì¬ í† í°ì˜ ë‹¤ìŒ í† í°ì…ë‹ˆë‹¤. ì´ê²ƒì€ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ë‘ ë²ˆì§¸ í† í°ë¶€í„° ì‹œì‘í•˜ì—¬ ë¼ë²¨ì„ í˜•ì„±í•©ë‹ˆë‹¤. ëª¨ë¸ì€ ì²« ë²ˆì§¸ í† í°ì— ëŒ€í•œ ì˜ˆì¸¡ì„ í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê·¸ ì „ì²´ ì…ë ¥ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ë¼ë²¨ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ë§ˆì§€ë§‰ ë¡œì§“ì„ ì˜ë¼ë‚´ì–´ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê° ìƒ˜í”Œë‹¹ ì†ì‹¤ì„ ê³„ì‚°í•˜ê³  ê° ìƒ˜í”Œì—ì„œ ëª¨ë“  í‚¤ì›Œë“œì˜ ë°œìƒì„ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ë°œìƒì„ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•œ ê°€ì¤‘ í‰ê· ì„ ê³„ì‚°í•©ë‹ˆë‹¤. í‚¤ì›Œë“œê°€ ì—†ëŠ” ëª¨ë“  ìƒ˜í”Œì„ ë²„ë¦¬ê³  ì‹¶ì§€ ì•Šìœ¼ë¯€ë¡œ ê°€ì¤‘ì¹˜ì— 1ì„ ì¶”ê°€í•©ë‹ˆë‹¤:

```py
from torch.nn import CrossEntropyLoss
import torch


def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
    # Shift so that tokens < n predict n
    shift_labels = inputs[..., 1:].contiguous()
    shift_logits = logits[..., :-1, :].contiguous()
    # Calculate per-token loss
    loss_fct = CrossEntropyLoss(reduction='none')
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    # Resize and average loss per sample
    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)
    # Calculate and scale weighting
    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(
        axis=[0, 2]
    )
    weights = alpha * (1.0 + weights)
    # Calculate weighted average
    weighted_loss = (loss_per_sample * weights).mean()
    return weighted_loss
```

ì´ ë©‹ì§„ ìƒˆë¡œìš´ ì†ì‹¤ í•¨ìˆ˜ë¡œ í•™ìŠµì„ ì‹œì‘í•˜ê¸° ì „ì— ë§ˆì§€ë§‰ìœ¼ë¡œ ëª‡ ê°€ì§€ë¥¼ ì¤€ë¹„í•´ì•¼ í•©ë‹ˆë‹¤:

- ë°ì´í„°ë¡œë”ë¥¼ ë§Œë“¤ì–´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.
- ê°€ì¤‘ì¹˜ ê°ì‡  ë§¤ê°œë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
- ë•Œë•Œë¡œ í‰ê°€í•˜ê³  ì‹¶ìœ¼ë¯€ë¡œ í‰ê°€ ì½”ë“œë¥¼ í•¨ìˆ˜ë¡œ ë˜í•‘í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

ë¨¼ì € ë°ì´í„°ë¡œë”ë¥¼ ì¤€ë¹„í•©ì‹œë‹¤. ë°ì´í„°ì…‹ì˜ í˜•ì‹ì„ `"torch"`ë¡œ ì„¤ì •í•˜ê³ , ì ì ˆí•œ ë°°ì¹˜ í¬ê¸°ë¡œ PyTorch `DataLoader`ì— ì „ë‹¬í•˜ë©´ ë©ë‹ˆë‹¤:

```py
from torch.utils.data.dataloader import DataLoader

tokenized_dataset.set_format("torch")
train_dataloader = DataLoader(tokenized_dataset["train"], batch_size=32, shuffle=True)
eval_dataloader = DataLoader(tokenized_dataset["valid"], batch_size=32)
```

ë‹¤ìŒìœ¼ë¡œ ë§¤ê°œë³€ìˆ˜ë¥¼ ê·¸ë£¹í™”í•˜ì—¬ ì˜µí‹°ë§ˆì´ì €ê°€ ì¶”ê°€ ê°€ì¤‘ì¹˜ ê°ì‡ ë¥¼ ë°›ì„ ë§¤ê°œë³€ìˆ˜ë¥¼ ì•Œ ìˆ˜ ìˆë„ë¡ í•´ì•¼ í•©ë‹ˆë‹¤. ë³´í†µ ëª¨ë“  í¸í–¥ ë° LayerNorm ê°€ì¤‘ì¹˜ ìš©ì–´ëŠ” ì´ì— í•´ë‹¹í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ìŒì€ ì´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤:

```py
weight_decay = 0.1


def get_grouped_params(model, no_decay=["bias", "LayerNorm.weight"]):
    params_with_wd, params_without_wd = [], []
    for n, p in model.named_parameters():
        if any(nd in n for nd in no_decay):
            params_without_wd.append(p)
        else:
            params_with_wd.append(p)
    return [
        {"params": params_with_wd, "weight_decay": weight_decay},
        {"params": params_without_wd, "weight_decay": 0.0},
    ]
```

ëª¨ë¸ì„ ì •ì˜í–ˆìœ¼ë¯€ë¡œ í‰ê°€ë¥¼ ìœ„í•œ ì½”ë“œë¥¼ í•¨ìˆ˜ë¡œ ì‘ì„±í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ê°„ë‹¨íˆ í‰ê°€ ë°ì´í„° ë¡œë”ë¥¼ í†µí•´ ëª¨ë¸ì„ ì‹¤í–‰í•˜ê³ , ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì—ì„œ ì†ì‹¤ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤:

```py
def evaluate():
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(batch["input_ids"], labels=batch["input_ids"])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    try:
        perplexity = torch.exp(loss)
    except OverflowError:
        perplexity = float("inf")
    return loss.item(), perplexity.item()
```

`evaluate()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì†ì‹¤ê³¼ [perplexity](/course/chapter7/3)ë¥¼ ë³´ê³ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒìœ¼ë¡œ ëª¨ë¸ì„ ë‹¤ì‹œ ì •ì˜í•˜ì—¬ ì²˜ìŒë¶€í„° ë‹¤ì‹œ í•™ìŠµí•©ë‹ˆë‹¤:

```py
model = GPT2LMHeadModel(config)
```

ê·¸ëŸ° ë‹¤ìŒ ë§¤ê°œë³€ìˆ˜ë¥¼ ê·¸ë£¹í™”í•˜ê³  ì˜µí‹°ë§ˆì´ì €ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì´ì „ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì¤‘ì¹˜ ê°ì‡ ì— ë”°ë¼ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

```py
from torch.optim import AdamW

optimizer = AdamW(get_grouped_params(model), lr=5e-4)
```

ì´ì œ ëª¨ë¸, ì˜µí‹°ë§ˆì´ì € ë° ë°ì´í„°ë¡œë”ë¥¼ ì¤€ë¹„í•˜ì—¬ í•™ìŠµì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤:

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

ğŸš¨ TPUì—ì„œ í•™ìŠµ ì¤‘ì¸ ê²½ìš°, ì—¬ê¸°ì„œë¶€í„°ì˜ ëª¨ë“  ì½”ë“œë¥¼ ì „ìš© í•™ìŠµ í•¨ìˆ˜ë¡œ ì´ë™í•´ì•¼ í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [Chapter 3](/course/chapter3)ì„ ì°¸ì¡°í•˜ì„¸ìš”.

</Tip>

ì´ì œ `train_dataloader`ë¥¼ `accelerator.prepare()`ì— ì „ì†¡í–ˆìœ¼ë¯€ë¡œ í•™ìŠµ ë‹¨ê³„ ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•­ìƒ ë°ì´í„° ë¡œë”ë¥¼ ì¤€ë¹„í•œ í›„ì— ì´ë ‡ê²Œ í•´ì•¼ í•©ë‹ˆë‹¤. ë°ì´í„° ë¡œë”ë¥¼ ì¤€ë¹„í•˜ëŠ” ê²ƒì´ ë©”ì„œë“œë¥¼ ë³€ê²½í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í•™ìŠµë¥ ë¶€í„° 0ê¹Œì§€ì˜ í´ë˜ì‹í•œ ì„ í˜• ì¼ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

```py
from transformers import get_scheduler

num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=1_000,
    num_training_steps

=num_training_steps,
)
```

ë§ˆì§€ë§‰ìœ¼ë¡œ, ëª¨ë¸ì„ Hubì— ì—…ë¡œë“œí•˜ë ¤ë©´ ì‘ì—… í´ë”ì—ì„œ `Repository` ê°ì²´ë¥¼ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤. ì´ë¯¸ ë¡œê·¸ì¸ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš° Hugging Face Hubì— ë¡œê·¸ì¸í•˜ì„¸ìš”. ëª¨ë¸ IDì—ì„œ ë¦¬í¬ì§€í† ë¦¬ ì´ë¦„ì„ ê²°ì •í•  ê²ƒì…ë‹ˆë‹¤(ììœ ë¡­ê²Œ `repo_name`ì„ ì›í•˜ëŠ” ëŒ€ë¡œ ë°”ê¾¸ì‹­ì‹œì˜¤. ì´ê²ƒì€ ì‚¬ìš©ì ì´ë¦„ì„ í¬í•¨í•´ì•¼ í•˜ë¯€ë¡œ í•¨ìˆ˜ `get_full_repo_name()`ì´ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/codeparrot-ds-accelerate'
```

ê·¸ëŸ° ë‹¤ìŒ í•´ë‹¹ ë¦¬í¬ì§€í† ë¦¬ë¥¼ ë¡œì»¬ í´ë”ì— ë³µì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš°, í•´ë‹¹ ë¡œì»¬ í´ë”ëŠ” ìš°ë¦¬ê°€ ì‘ì—…í•˜ëŠ” ë¦¬í¬ì§€í† ë¦¬ì˜ ê¸°ì¡´ ë³µì œë³¸ì´ì–´ì•¼ í•©ë‹ˆë‹¤:

```py
output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

`repo.push_to_hub()` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ `output_dir`ì— ì €ì¥ëœ ëª¨ë“  ê²ƒì„ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ê° ì—í¬í¬ì˜ ì¤‘ê°„ ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

í•™ìŠµí•˜ê¸° ì „ì—, í‰ê°€ í•¨ìˆ˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•˜ëŠ”ì§€ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸í•´ ë´…ì‹œë‹¤:

```py
evaluate()
```

```python out
(10.934126853942871, 56057.14453125)
```

ì†ì‹¤ê³¼ í¼í”Œë ‰ì„œí‹°ì— ëŒ€í•œ ì´ ê°’ì€ ë§¤ìš° ë†’ì§€ë§Œ, ëª¨ë¸ì„ ì•„ì§ í•™ìŠµí•˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ë†€ëì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤. ì´ì œ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì˜ í•µì‹¬ ë¶€ë¶„ì¸ í•™ìŠµ ë£¨í”„ë¥¼ ì‘ì„±í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. í•™ìŠµ ë£¨í”„ì—ì„œëŠ” ë°ì´í„° ë¡œë”ë¥¼ ë°˜ë³µí•˜ê³  ë°°ì¹˜ë¥¼ ëª¨ë¸ì— ì „ë‹¬í•©ë‹ˆë‹¤. ë¡œì§“ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì •ì˜ ì†ì‹¤ í•¨ìˆ˜ë¥¼ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì†ì‹¤ì„ ì ì ˆí•œ ê¸‰ê²©í•œ ì¶•ì†Œë¡œ ìŠ¤ì¼€ì¼ë§í•˜ì—¬ ë” ë§ì€ ë‹¨ê³„ë¥¼ ì§‘ê³„í•  ë•Œ ë” í° ì†ì‹¤ì„ ë§Œë“¤ì§€ ì•ŠìŠµë‹ˆë‹¤. ìµœì í™”í•˜ê¸° ì „ì— ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ í´ë¦¬í•‘í•˜ì—¬ ìˆ˜ë ´ì„ ê°œì„ í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ìƒˆë¡œìš´ `evaluate()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€ ì„¸íŠ¸ì—ì„œ ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤:

```py
from tqdm.notebook import tqdm

gradient_accumulation_steps = 8
eval_steps = 5_000

model.train()
completed_steps = 0
for epoch in range(num_train_epochs):
    for step, batch in tqdm(
        enumerate(train_dataloader, start=1), total=num_training_steps
    ):
        logits = model(batch["input_ids"]).logits
        loss = keytoken_weighted_loss(batch["input_ids"], logits, keytoken_ids)
        if step % 100 == 0:
            accelerator.print(
                {
                    "samples": step * samples_per_step,
                    "steps": completed_steps,
                    "loss/train": loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        if step % gradient_accumulation_steps == 0:
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += 1
        if (step % (eval_steps * gradient_accumulation_steps)) == 0:
            eval_loss, perplexity = evaluate()
            accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            if accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=f"Training in progress step {step}", blocking=False
                )
```

ì´ê²ƒìœ¼ë¡œ ëª¨ë“  ì¤€ë¹„ê°€ ëë‚¬ìŠµë‹ˆë‹¤. ì´ì œ GPT-2ì™€ ê°™ì€ ì¸ê³¼ ì–¸ì–´ ëª¨ë¸ì— ëŒ€í•œ ì‚¬ìš©ì ì •ì˜ í•™ìŠµ ë£¨í”„ë¥¼ ì§ì ‘ ì‘ì„±í•˜ì—¬ ì›í•˜ëŠ” ëŒ€ë¡œ ì‚¬ìš©ì ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<Tip>

âœï¸ **í•´ë³´ê¸°!** ì‚¬ìš©ì ì •ì˜ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì—¬ ì‚¬ìš© ì‚¬ë¡€ì— ë§ê²Œ ë§ì¶¤í™”í•˜ê±°ë‚˜, í•™ìŠµ ë£¨í”„ì— ë‹¤ë¥¸ ì‚¬ìš©ì ì •ì˜ ë‹¨ê³„ë¥¼ ì¶”ê°€í•´ ë³´ì„¸ìš”.

</Tip>

<Tip>

âœï¸ **í•´ë³´ê¸°!** ê¸´ í•™ìŠµ ì‹¤í—˜ì„ ì‹¤í–‰í•  ë•ŒëŠ” TensorBoard ë˜ëŠ” Weights & Biasesì™€ ê°™ì€ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¤‘ìš”í•œ ë©”íŠ¸ë¦­ì„ ë¡œê·¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. í•™ìŠµ ë£¨í”„ì— ì ì ˆí•œ ë¡œê¹…ì„ ì¶”ê°€í•˜ì—¬ í•­ìƒ í•™ìŠµ ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

</Tip>

{/if}
