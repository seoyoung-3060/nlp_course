<FrameworkSwitchCourse {fw} />

# 처음부터 인과 언어 모델을 훈련시키기[[training-a-causal-language-model-from-scratch]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_tf.ipynb"},
]} />

{/if}

지금까지 우리는 주로 사전 훈련된 모델을 사용하고 그들의 가중치를 재사용하여 새로운 사용 사례에 대해 세세한 조정을 해왔습니다. 우리는 [1장](/course/chapter1)에서 본 것처럼 이를 _전이 학습_이라고 일반적으로 부르며, 희소한 라벨이 있는 대부분의 실제 사용 사례에 Transformer 모델을 적용하는 매우 성공적인 전략입니다. 이번 장에서는 다른 접근 방식을 취하여 완전히 새로운 모델을 처음부터 훈련시키겠습니다. 이것은 귀하가 많은 데이터를 가지고 있고 사용 가능한 모델에 사용된 사전 훈련 데이터와 매우 다른 경우에 좋은 접근 방식입니다. 그러나 이것은 사전 훈련된 언어 모델을 단순히 세세히 조정하는 것보다 언어 모델을 사전 훈련하는 데 상당히 더 많은 계산 리소스를 필요로 합니다. 새로운 모델을 훈련시킬 수 있는 예는 음악 노트로 이루어진 데이터 세트, DNA와 같은 분자 서열, 또는 프로그래밍 언어가 있습니다. 후자는 TabNine와 GitHub의 Copilot과 같은 도구들 덕분에 최근에 인기를 얻었습니다. 이들은 OpenAI의 Codex 모델에서 파생된 긴 코드 시퀀스를 생성할 수 있습니다. 이러한 텍스트 생성 작업은 GPT-2와 같은 자기 회귀형 또는 인과 언어 모델로 처리하는 것이 가장 좋습니다.

이 섹션에서는 코드 생성 모델의 축소판을 만들 것입니다: 우리는 Python 코드의 일줄 완성에 초점을 맞출 것이며, 전체 함수나 클래스 대신 Python 코드의 하위 집합을 사용할 것입니다. Python 데이터를 다룰 때 주로 `matplotlib`, `seaborn`, `pandas`, `scikit-learn` 라이브러리로 이루어진 Python 데이터 과학 스택과 자주 상호 작용하게 됩니다. 이러한 프레임워크를 사용할 때 특정 명령을 찾아야 할 때가 많기 때문에 이러한 호출을 자동으로 완성하는 모델을 사용할 수 있다면 좋을 것입니다.

<Youtube id="Vpjb1lu0MDk"/>

[6장](/course/chapter6)에서 우리는 Python 소스 코드를 처리하기 위한 효율적인 토크나이저를 생성했지만, 여전히 모델을 사전 훈련할 대규모 데이터 세트가 필요합니다. 여기서 우리는 GitHub 리포지토리에서 파생된 Python 코드 코퍼스에 우리의 토크나이저를 적용할 것입니다. 그런 다음 `Trainer` API와 🤗 가속기를 사용하여 모델을 훈련할 것입니다. 시작해봅시다!

<iframe src="https://course-demos-codeparrot-ds.hf.space" frameBorder="0" height="300" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

이것은 실제로 이 섹션에서 보여진 코드를 사용하여 훈련하고 Hub에 업로드된 모델을 전시하는 것입니다. 여기서 찾을 수 있습니다 [here](https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28). 텍스트 생성에 약간의 무작위성이 발생하기 때문에 결과가 약간 다를 수 있습니다.
 
## 데이터 수집[[gathering-the-data]]

Python 코드는 GitHub와 같은 코드 리포지토리에서 풍부하게 사용할 수 있으며, 우리는 모든 Python 리포지토리를 스크랩하여 데이터 세트를 생성할 수 있습니다. 이것은 [Transformers textbook](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/)에서 대규모 GPT-2 모델을 사전 훈련하기 위해 취한 접근 방식입니다. 약 180GB 크기의 GitHub 덤프인 `codeparrot`이라는 약 2000만 개의 Python 파일이 들어있는 데이터 세트를 사용하여 저자들은 데이터 세트를 구축한 후 이를 [Hugging Face Hub](https://huggingface.co/datasets/transformersbook/codeparrot)에 공유했습니다.

그러나 전체 코퍼스에서 훈련하는 것은 시간과 컴퓨팅 리소스가 많이 소모되므로 Python 데이터 과학 스택에 관련된 데이터 세트의 하위 집합만 필요합니다. 그러므로 먼저 `codeparrot` 데이터 세트를 필터링하여 이 스택에 관련된 모든 파일을 가져와야 합니다. 데이터 세트의 크기 때문에 다운로드하는 것을 피해야 하기 때문에 이를 스트리밍 기능을 사용하여 실시간으로 필터링할 것입니다. 앞에서 언급한 라이브러리를 사용하여 코드 샘플을 필터링하는 데 도움을 주기 위해 다음 함수를 사용할 것입니다:

```py
def any_keyword_in_string(string, keywords):
    for keyword in keywords:
        if keyword in string:
            return True
    return False
```

두 가지 예제에서 이것을 테스트할 수 있습니다:

```py
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]
example_1 = "import numpy as np"
example_2 = "import pandas as pd"

print(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)
```

```python out
False True
```

이 함수를 사용하여 스트리밍 데이터 세트를 만들고 우리가 원하는 요소를 필터링할 수 있는 함수를 만들 수 있습니다:

```py
from collections import defaultdict
from tqdm import tqdm
from datasets import Dataset


def filter_streaming_dataset(dataset, filters):
    filtered_dict = defaultdict(list)
    total = 0
    for sample in tqdm(iter(dataset)):
        total += 1
        if any_keyword_in_string(sample["content"], filters):
            for k, v in sample.items():
                filtered_dict[k].append(v)
    print(f"{len(filtered_dict['content'])/total:.2%} of data after filtering.")
    return Dataset.from_dict(filtered_dict)
```

그런 다음 이 함수를 단순히 스트리밍 데이터 세트에 적용할 수 있습니다:


```py
# 이 셀을 실행하는 데 매우 오랜 시간이 걸리므로 이를 건너뛰고 다음으로 넘어가는 것이 좋습니다!
from datasets import load_dataset

split = "train"  # "valid"
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]

data = load_dataset(f"transformersbook/codeparrot-{split}", split=split, streaming=True)
filtered_data = filter_streaming_dataset(data, filters)
```

```python out
3.26% of data after filtering.
```
이렇게 하면 원래 데이터 세트의 약 3%만 남게 됩니다. 이것은 여전히 상당히 큰 데이터 세트입니다. 결과 데이터 세트는 6GB이고 60만 개의 Python 스크립트로 구성됩니다!

전체 데이터 세트를 필터링하는 데는 사용하는 기계 및 대역폭에 따라 2~3시간이 소요될 수 있습니다. 이러한 긴 과정을 직접 수행하지 않으려는 경우 필터링된 데이터 세트를 다운로드할 수 있도록 Hub에서 제공합니다:

```py
from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="validation")

raw_datasets = DatasetDict(
    {
        "train": ds_train,  # .shuffle().select(range(50000)),
        "valid": ds_valid,  # .shuffle().select(range(500))
    }
)

raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 606720
    })
    valid: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 3322
    })
})
```

<Tip>

언어 모델을 사전 훈련하는 데 시간이 걸릴 것입니다. 먼저 위의 두 부분 라인의 주석을 해제하여 데이터의 샘플에서 훈련 루프를 실행하고 훈련이 성공적으로 완료되고 모델이 저장되는지 확인하십시오. 훈련 루프의 끝에 폴더를 만들거나 오타가 있어서 마지막 단계에서 훈련 실행이 실패하는 것보다 더 좋습니다!

</Tip>

데이터 세트의 예제를 살펴보겠습니다. 각 필드의 처음 200자만 표시하겠습니다:

```py
for key in raw_datasets["train"][0]:
    print(f"{key.upper()}: {raw_datasets['train'][0][key][:200]}")
```

```python out
'REPO_NAME: kmike/scikit-learn'
'PATH: sklearn/utils/__init__.py'
'COPIES: 3'
'SIZE: 10094'
'''CONTENT: """
The :mod:`sklearn.utils` module includes various utilites.
"""

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause'''
```

`content` 필드가 우리의 모델이 훈련할 코드를 포함하고 있음을 볼 수 있습니다. 이제 데이터 세트가 있으므로 사전 훈련에 적합한 형식으로 텍스트를 준비해야 합니다.

## 데이터 세트 준비하기[[preparing-the-dataset]]

<Youtube id="ma1TrR7gE7I"/>

첫 번째 단계는 데이터를 토큰화하여 훈련에 사용할 수 있도록하는 것입니다. 주로 짧은 함수 호출을 자동으로 완성하기 위한 것이므로 컨텍스트 크기를 상대적으로 작게 유지할 것입니다. 이것은 모델을 훨씬 더 빠르게 훈련할 수 있게 하고 메모리를 훨씬 적게 필요로 합니다. 어플리케이션에서 더 많은 컨텍스트를 갖는 것이 중요하다면(예를 들어, 파일의 함수 정의를 기반으로 모델이 단위 테스트를 작성하도록 하려면), 해당 숫자를 증가시키되 이는 더 많은 GPU 메모리를 사용한다는 것을 염두에 두세요. 지금은 컨텍스트 크기를 128 토큰으로 고정하겠습니다. 이는 GPT-2나 GPT-3에서 사용되는 1,024 또는 2,048과는 달리 상대적으로 작습니다.

대부분의 문서는 128개 이상의 토큰을 포함하므로 최대 길이로 입력을 자르면 데이터 세트의 많은 부분이 사라집니다. 대신 `return_overflowing_tokens` 옵션을 사용하여 전체 입력을 토큰화하고 여러 청크로 나눌 수 있습니다. 우리는 [6장](/course/chapter6/4)에서 했던 것처럼 이것을 할 것입니다. 또한 `return_length` 옵션을 사용하여 자동으로 생성된 각 청크의 길이를 반환할 것입니다. 마지막 청크는 일반적으로 컨텍스트 크기보다 작을 것이므로 패딩 문제를 피하기 위해 이러한 조각들을 제거합니다. 우리가 이미 충분한 데이터를 가지고 있기 때문에 이러한 조각들은 실제로 필요하지 않습니다.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg" alt="Chunking a large texts in several pieces."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts-dark.svg" alt="Chunking a large texts in several pieces."/>
</div>

이 작업이 어떻게 작동하는지를 정확히 살펴보겠습니다. 처음 두 예제를 살펴보면 다음과 같습니다:

```py
from transformers import AutoTokenizer

context_length = 128
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
    raw_datasets["train"][:2]["content"],
    truncation=True,
    max_length=context_length,
    return_overflowing_tokens=True,
    return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")
```

```python out
Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

이렇게 하면 우리는 두 예제에서 총 34개의 세그먼트를 얻게 됩니다. 청크의 길이를 살펴보면, 두 문서 모두 끝에 있는 청크가 128토큰 미만임을 알 수 있습니다(각각 117과 41). 이들은 우리가 가진 총 청크 중에 작은 일부를 나타냅니다. 따라서 이를 안전하게 버릴 수 있습니다. `overflow_to_sample_mapping` 필드를 사용하여 어떤 청크가 어떤 입력 샘플에 속하는지도 재구성할 수 있습니다.

이 작업을 수행하는 데에는 🤗 데이터셋의 `Dataset.map()` 함수의 유용한 기능을 활용하고 있습니다. 이 함수는 일대일 매핑을 요구하지 않으며, 하나의 입력 배치보다 더 많거나 더 적은 요소를 포함하는 배치를 생성할 수 있습니다. 데이터 증강 또는 데이터 필터링과 같은 작업을 수행할 때 유용합니다. 우리의 경우에는 각 요소를 지정된 문맥 크기의 청크로 토큰화할 때마다 각 문서에서 많은 샘플을 생성합니다. 다만, 기존 열을 삭제해야 하므로 크기가 충돌하는 것에 유의해야 합니다. 이를 유지하려면 해당 열을 적절히 반복하고 `Dataset.map()` 호출 내에서 반환하면 됩니다.

```py
def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 16702061
    })
    valid: Dataset({
        features: ['input_ids'],
        num_rows: 93164
    })
})
```

이제 각각 128 토큰씩 1,670만 개의 예제가 있으며, 총 21억 토큰에 해당합니다. 참고로 OpenAI의 GPT-3와 Codex 모델은 각각 300억과 1000억 토큰에 대해 훈련되었으며, Codex 모델은 GPT-3 체크포인트에서 초기화되었습니다. 이 섹션의 목표는 이러한 모델과 경쟁하는 것이 아니라 데이터 과학자를 위한 빠른 자동 완성 기능을 제공하는 축소판을 만드는 것입니다.

데이터 세트가 준비되었으므로 이제 모델을 설정해 봅시다!

<Tip>

✏️ **해보세요!** 여기서는 작은 컨텍스트 창을 사용하므로 컨텍스트 크기보다 작은 모든 청크를 제거하는 것이 큰 문제가 되지 않았습니다. 컨텍스트 크기를 늘리면(또는 짧은 문서의 말뭉치를 사용하는 경우) 버려진 청크의 비율도 늘어납니다. 데이터를 준비하는 더 효율적인 방법은 모든 토큰화된 샘플을 `eos_token_id` 토큰 사이에 결합한 다음 연결된 시퀀스에서 청크를 생성하는 것입니다. 연습으로 `tokenize()` 함수를 수정하여 해당 방법을 사용하십시오. 토큰화에서 `truncation=False`를 설정하고 다른 인수를 토크나이저에서 제거해야 전체 토큰 ID 시퀀스를 얻을 수 있습니다.

</Tip>


## 새 모델 초기화[[initializing-a-new-model]]

첫 번째 단계는 새로운 GPT-2 모델을 초기화하는 것입니다. 우리는 모델의 크기를 GPT-2 작은 모델과 같게 하기 위해 우리 모델에 대한 동일한 구성을 사용할 것입니다. 따라서 사전 훈련된 구성을 로드하고 토크나이저 크기가 모델 어휘 크기와 일치하도록하고 `bos`와 `eos`(시퀀스의 시작과 끝) 토큰 ID를 전달할 것입니다:

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

이 구성을 사용하여 새 모델을 로드할 수 있습니다. 이것은 우리가 실제로 모델을 초기화하기 때문에 `from_pretrained()` 함수를 사용하지 않는 첫 번째 경우입니다:

```py
model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")
```

```python out
GPT-2 size: 124.2M parameters
```

{:else}

```py
from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

그런 다음이 구성을 사용하여 새 모델을 로드할 수 있습니다. 이것은 실제로 우리가 모델을 초기화하기 때문에 우리가 `from_pretrained()` 함수를 사용하지 않는 첫 번째 경우입니다:

```py
model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  # Builds the model
model.summary()
```

```python out
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
transformer (TFGPT2MainLayer multiple                  124242432 
=================================================================
Total params: 124,242,432
Trainable params: 124,242,432
Non-trainable params: 0
_________________________________________________________________
```

{/if}

우리 모델은 조정해야 할 124M개의 매개변수를 갖고 있습니다. 훈련을 시작하기 전에 배치를 생성하는 데이터 콜레이터를 설정해야합니다. 특히 언어 모델링에 특화된 `DataCollatorForLanguageModeling` 콜레이터를 사용할 수 있습니다(이름에서 약간 암시됩니다). 배치를 쌓고 패딩하는 것 외에도 이 콜레이터는 언어 모델 레이블을 생성하는데 사용됩니다. 인과 언어 모델링에서 입력은 레이블 역할을 하기도 합니다(하나의 요소만 시프트된 것). 이 데이터 콜레이터는 훈련 중에 레이블을 생성하여 `input_ids`를 중복하여 생성할 필요가 없도록합니다.

`DataCollatorForLanguageModeling`은 마스크 언어 모델링(MLM) 및 인과 언어 모델링(CLM)을 모두 지원합니다. 기본적으로 이 콜레이터는 MLM을 위해 데이터를 준비하지만 `mlm=False` 인수를 설정하여 CLM으로 전환할 수 있습니다:

{#if fw === 'pt'}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
```

{:else}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")
```

{/if}

예제를 살펴보겠습니다:

```py
out = data_collator([tokenized_datasets["train"][i] for i in range(5)])
for key in out:
    print(f"{key} shape: {out[key].shape}")
```

{#if fw === 'pt'}

```python out
input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])
```

{:else}

```python out
input_ids shape: (5, 128)
attention_mask shape: (5, 128)
labels shape: (5, 128)
```

{/if}

예제가 쌓여 있고 모든 텐서가 동일한 모양을 가지고 있는 것을 볼 수 있습니다.

{#if fw === 'tf'}

이제 `prepare_tf_dataset()` 메서드를 사용하여 데이터 세트를 TensorFlow 데이터 세트로 변환할 수 있습니다. 위에서 생성한 데이터 콜레이터와 함께:

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_dataset["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_dataset["valid"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)
```

{/if}

<Tip warning={true}>

⚠️ 입력과 레이블을 정렬하여 맞추는 것은 모델 내부에서 발생하므로 데이터 콜레이터는 레이블을 생성하기 위해 입력을 단순히 복사합니다.

</Tip>


이제 우리는 모델을 실제로 훈련할 모든 준비를 마쳤습니다. 이제 훈련을 시작하기 전에 Hugging Face에 로그인해야 합니다. 노트북에서 작업 중이라면 다음 유틸리티 함수를 사용하여 로그인할 수 있습니다:

```python
from huggingface_hub import notebook_login

notebook_login()
```

이것은 Hugging Face 로그인 자격 증명을 입력할 수 있는 위젯을 표시합니다.

노트북에서 작업하지 않는 경우 터미널에서 다음 줄을 입력하면됩니다:

```bash
huggingface-cli login
```

{#if fw === 'pt'}

나머지 작업은 훈련 인수를 구성하고 `Trainer`를 실행하는 것뿐입니다. 코사인 학습률 스케줄과 일부 웜업, 그리고 효과적인 배치 크기인 256(`per_device_train_batch_size` * `gradient_accumulation_steps`)를 사용할 것입니다. 그라디언트 누적은 단일 배치가 메모리에 맞지 않을 때 사용되며 여러 전방향/후방향 패스를 통해 점진적으로 그라디언트를 누적합니다. 이것을 🤗 Accelerate와 함께 훈련 루프를 만들 때 볼 것입니다.

```py
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="codeparrot-ds",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)
```

이제 `Trainer`를 시작하고 훈련이 완료될 때까지 기다리면 됩니다. 훈련 세트의 전체 또는 일부에 실행하는지에 따라 시간이 달라집니다. 전체 데이터셋에 대해 실행하는 경우 20시간이 걸릴 수 있으며, 일부 데이터셋에 대해 실행하는 경우 2시간이 걸릴 수 있습니다. 따라서 여유 있게 몇 잔의 커피를 마시고 좋은 책을 읽으면 됩니다!

```py
trainer.train()
```

훈련이 완료되면 모델과 토크나이저를 Hub에 업로드할 수 있습니다.

```py
trainer.push_to_hub()
```

{:else}

나머지 할 일은 훈련 하이퍼파라미터를 구성하고 `compile()` 및 `fit()`을 호출하는 것뿐입니다. 훈련의 안정성을 높이기 위해 일정을 사용할 것입니다.

```py
from transformers import create_optimizer
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

이제 `model.fit()`을 호출하고 훈련이 완료될 때까지 기다리면 됩니다. 전체 데이터셋 또는 일부 데이터셋에서 실행하느냐에 따라 훈련에 걸리는 시간이 달라집니다. 전체 데이터셋으로 실행하는 경우 20시간이 걸릴 수 있으니, 좋은 책과 커피를 준비하세요! 훈련이 완료되면 모델과 토크나이저를 Hub에 업로드할 수 있습니다.

```py
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="codeparrot-ds", tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```
<Tip>

✏️ **해보세요!** `TrainingArguments`에 몇 줄의 코드만 추가하면 원시 텍스트에서 GPT-2를 학습시킬 수 있습니다. 여러분의 데이터셋으로 시도해보고 얼마나 좋은 결과를 얻을 수 있는지 확인해보세요!

</Tip>

<Tip>

{#if fw === 'pt'}

💡 여러분의 머신에 다중 GPU가 있는 경우 코드를 실행해보세요. `Trainer`는 자동으로 여러 머신을 관리하며, 이렇게 하면 학습 속도가 크게 향상될 수 있습니다.

{:else}

💡 여러분의 머신에 다중 GPU가 있는 경우 `MirroredStrategy` 컨텍스트를 사용하여 학습 속도를 상당히 높일 수 있습니다. `tf.distribute.MirroredStrategy` 객체를 만들고, `to_tf_dataset()` 또는 `prepare_tf_dataset()` 메서드 및 모델 생성 및 `fit()` 호출이 모두 해당 `scope()` 컨텍스트 내에서 실행되도록 해야 합니다. 자세한 내용은 [여기](https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit)에서 확인할 수 있습니다.

{/if}

</Tip>

## 파이프라인을 사용한 코드 생성[[code-generation-with-a-pipeline]]

이제가 모델의 실제 작동 여부를 확인하는 순간입니다! 로그에서 손실이 꾸준히 감소했음을 볼 수 있지만, 모델을 테스트하려면 몇 가지 프롬프트에서 얼마나 잘 작동하는지 살펴보겠습니다. 이를 위해 모델을 텍스트 생성 `파이프라인`으로 래핑하고, GPU가 있는 경우 빠른 생성을 위해 GPU에 넣겠습니다:

{#if fw === 'pt'}

```py
import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
    "text-generation", model="huggingface-course/codeparrot-ds", device=device
)
```

{:else}

```py
from transformers import pipeline

course_model = TFGPT2LMHeadModel.from_pretrained("huggingface-course/codeparrot-ds")
course_tokenizer = AutoTokenizer.from_pretrained("huggingface-course/codeparrot-ds")
pipe = pipeline(
    "text-generation", model=course_model, tokenizer=course_tokenizer, device=0
)
```

{/if}

간단한 산점도를 만드는 간단한 작업으로 시작해보겠습니다:

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
plt.scatter(x, y)

# create scatter
```

결과가 올바르게 보입니다. `pandas` 작업에도 작동하는지 확인해보겠습니다. 두 배열에서 `DataFrame`을 만들 수 있는지 살펴보겠습니다:

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for
```

좋습니다. 올바른 답변입니다. 그러나 그럼에도 불구하고 열 `x`를 다시 삽입합니다. 생성된 토큰 수가 제한되어 있으므로 다음 `for` 루프가 잘려 나갑니다. 더 복잡한 작업을 수행하고 모델이 `groupby` 작업을 도와주는지 확인해보겠습니다:

```py
txt = """\
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
profession = df.groupby(['profession']).mean()

# compute the
```

꽤 괜찮습니다. 이렇게 하면 올바른 방법입니다. 마지막으로 `scikit-learn`을 사용하여 랜덤 포레스트 모델을 설정할 수 있는지 확인해보겠습니다:

```py
txt = """
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf
```

{#if fw === 'tf'}

이러한 몇 가지 예를 살펴보면 모델이 Python 데이터 과학 스택의 일부 구문을 학습한 것으로 보입니다. 물론 실제 세계에 배포하기 전에 모델을 더 철저하게 평가해야 하지만, 이는 여전히 인상적인 프로토타입입니다.

{:else}

이러한 몇 가지 예를 살펴보면 모델이 Python 데이터 과학 스택의 일부 구문을 학습한 것으로 보입니다(물론, 실제 세계에서 모델을 배포하기 전에 더 철저히 평가해야 합니다). 때로는 주어진 사용 사례에 필요한 성능을 달성하기 위해 모델 학습을 더 많이 사용자 정의해야 할 수 있습니다. 예를 들어 배치 크기를 동적으로 업데이트하거나 조건부 학습 루프를 사용하여 나쁜 예제를 실시간으로 건너뛰는 경우 어떻게 해야 할까요? `Trainer`를 서브 클래스화하고 필요한 변경 사항을 추가하는 것이 한 가지 옵션일 수 있지만, 때로는 학습 루프를 처음부터 작성하는 것이 더 간단할 수 있습니다. 여기서 🤗 Accelerate가 필요합니다.

{/if}

{#if fw === 'pt'}

## 🤗 Accelerate로 학습[[training-with-accelerate]]

`Trainer`를 사용하여 모델을 학습하는 방법을 살펴보았습니다. 이를 통해 약간의 사용자 정의가 가능합니다. 그러나 때로는 학습 루프를 완전히 제어하거나 어떤 독특한 변경 사항을 만들고 싶을 수 있습니다. 이러한 경우 🤗 Accelerate가 좋은 선택이며, 이 섹션에서는 이를 사용하여 모델을 학습하는 단계를 살펴보겠습니다. 좀 더 흥미로운 점은 학습 루프에 추가로 변형을 추가할 것입니다.

<Youtube id="Hm8_PgVTFuc"/>

주로 데이터 과학 라이브러리에 대한 합리적인 자동 완성을 원하기 때문에, 이러한 라이브러리를 더 많이 활용하는 학습 샘플에 더 많은 가중치를 부여하는 것이 합리적입니다. 우리는 `plt`, `pd`, `sk`, `fit` 및 `predict`와 같은 키워드를 통해 이러한 예제를 쉽게 식별할 수 있습니다. 이것들은 각각 `matplotlib.pyplot`, `pandas`, `sklearn`의 가장 빈번한 import 이름 및 후자의 fit/predict 패턴입니다. 이들을 하나의 토큰으로 표현한다면 입력 시퀀스에 나타나는지 쉽게 확인할 수 있습니다. 토큰에는 공백 접두사가 있을 수 있으므로 토크나이저 어휘에서 해당 버전도 확인해야 합니다. 이것이 작동하는지 확인하기 위해 여러 토큰을 테스트합니다. 이 중 하나는 여러 토큰으로 분할되어야 합니다:

```py
keytoken_ids = []
for keyword in [
    "plt",
    "pd",
    "sk",
    "fit",
    "predict",
    " plt",
    " pd",
    " sk",
    " fit",
    " predict",
    "testtest",
]:
    ids = tokenizer([keyword]).input_ids[0]
    if len(ids) == 1:
        keytoken_ids.append(ids[0])
    else:
        print(f"Keyword has not single token: {keyword}")
```

```python out
'Keyword has not single token: testtest'
```

정상적으로 작동하는 것 같습니다! 이제 입력 시퀀스, 로짓 및 방금 선택한 키 토큰을 사용하는 사용자 정의 손실 함수를 작성할 수 있습니다. 먼저 로짓 및 입력을 정렬해야 합니다. 입력 시퀀스를 오른쪽으로 한 칸 이동하면 라벨이 형성되므로, 라벨은 현재 토큰의 다음 토큰입니다. 이것은 입력 시퀀스의 두 번째 토큰부터 시작하여 라벨을 형성합니다. 모델은 첫 번째 토큰에 대한 예측을 하지 않으므로 그 전체 입력 시퀀스에 대한 라벨이 필요하지 않습니다. 그런 다음 마지막 로짓을 잘라내어 사용하지 않습니다. 이를 통해 각 샘플당 손실을 계산하고 각 샘플에서 모든 키워드의 발생을 계산할 수 있습니다. 마지막으로, 발생을 가중치로 사용하여 모든 샘플에 대한 가중 평균을 계산합니다. 키워드가 없는 모든 샘플을 버리고 싶지 않으므로 가중치에 1을 추가합니다:

```py
from torch.nn import CrossEntropyLoss
import torch


def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
    # Shift so that tokens < n predict n
    shift_labels = inputs[..., 1:].contiguous()
    shift_logits = logits[..., :-1, :].contiguous()
    # Calculate per-token loss
    loss_fct = CrossEntropyLoss(reduction='none')
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    # Resize and average loss per sample
    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)
    # Calculate and scale weighting
    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(
        axis=[0, 2]
    )
    weights = alpha * (1.0 + weights)
    # Calculate weighted average
    weighted_loss = (loss_per_sample * weights).mean()
    return weighted_loss
```

이 멋진 새로운 손실 함수로 학습을 시작하기 전에 마지막으로 몇 가지를 준비해야 합니다:

- 데이터로더를 만들어 배치 단위로 데이터를 로드해야 합니다.
- 가중치 감쇠 매개변수를 설정해야 합니다.
- 때때로 평가하고 싶으므로 평가 코드를 함수로 래핑하는 것이 좋습니다.

먼저 데이터로더를 준비합시다. 데이터셋의 형식을 `"torch"`로 설정하고, 적절한 배치 크기로 PyTorch `DataLoader`에 전달하면 됩니다:

```py
from torch.utils.data.dataloader import DataLoader

tokenized_dataset.set_format("torch")
train_dataloader = DataLoader(tokenized_dataset["train"], batch_size=32, shuffle=True)
eval_dataloader = DataLoader(tokenized_dataset["valid"], batch_size=32)
```

다음으로 매개변수를 그룹화하여 옵티마이저가 추가 가중치 감쇠를 받을 매개변수를 알 수 있도록 해야 합니다. 보통 모든 편향 및 LayerNorm 가중치 용어는 이에 해당하지 않습니다. 다음은 이를 수행하는 방법입니다:

```py
weight_decay = 0.1


def get_grouped_params(model, no_decay=["bias", "LayerNorm.weight"]):
    params_with_wd, params_without_wd = [], []
    for n, p in model.named_parameters():
        if any(nd in n for nd in no_decay):
            params_without_wd.append(p)
        else:
            params_with_wd.append(p)
    return [
        {"params": params_with_wd, "weight_decay": weight_decay},
        {"params": params_without_wd, "weight_decay": 0.0},
    ]
```

모델을 정의했으므로 평가를 위한 코드를 함수로 작성하는 것이 좋습니다. 이 함수는 간단히 평가 데이터 로더를 통해 모델을 실행하고, 모든 프로세스에서 손실을 수집합니다:

```py
def evaluate():
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(batch["input_ids"], labels=batch["input_ids"])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    try:
        perplexity = torch.exp(loss)
    except OverflowError:
        perplexity = float("inf")
    return loss.item(), perplexity.item()
```

`evaluate()` 함수를 사용하여 일정한 간격으로 손실과 [perplexity](/course/chapter7/3)를 보고할 수 있습니다. 다음으로 모델을 다시 정의하여 처음부터 다시 학습합니다:

```py
model = GPT2LMHeadModel(config)
```

그런 다음 매개변수를 그룹화하고 옵티마이저를 정의합니다. 이전과 마찬가지로 매개변수를 가중치 감쇠에 따라 분리하는 함수를 사용합니다:

```py
from torch.optim import AdamW

optimizer = AdamW(get_grouped_params(model), lr=5e-4)
```

이제 모델, 옵티마이저 및 데이터로더를 준비하여 학습을 시작할 준비가 되었습니다:

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

🚨 TPU에서 학습 중인 경우, 여기서부터의 모든 코드를 전용 학습 함수로 이동해야 합니다. 자세한 내용은 [Chapter 3](/course/chapter3)을 참조하세요.

</Tip>

이제 `train_dataloader`를 `accelerator.prepare()`에 전송했으므로 학습 단계 수를 계산할 수 있습니다. 항상 데이터 로더를 준비한 후에 이렇게 해야 합니다. 데이터 로더를 준비하는 것이 메서드를 변경하기 때문입니다. 우리는 학습률부터 0까지의 클래식한 선형 일정을 사용합니다:

```py
from transformers import get_scheduler

num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=1_000,
    num_training_steps

=num_training_steps,
)
```

마지막으로, 모델을 Hub에 업로드하려면 작업 폴더에서 `Repository` 객체를 만들어야 합니다. 이미 로그인되어 있지 않은 경우 Hugging Face Hub에 로그인하세요. 모델 ID에서 리포지토리 이름을 결정할 것입니다(자유롭게 `repo_name`을 원하는 대로 바꾸십시오. 이것은 사용자 이름을 포함해야 하므로 함수 `get_full_repo_name()`이 하는 것입니다):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/codeparrot-ds-accelerate'
```

그런 다음 해당 리포지토리를 로컬 폴더에 복제할 수 있습니다. 이미 존재하는 경우, 해당 로컬 폴더는 우리가 작업하는 리포지토리의 기존 복제본이어야 합니다:

```py
output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

`repo.push_to_hub()` 메서드를 호출하여 `output_dir`에 저장된 모든 것을 업로드할 수 있습니다. 이렇게 하면 각 에포크의 중간 모델을 업로드하는 데 도움이 됩니다.

학습하기 전에, 평가 함수가 올바르게 작동하는지 빠르게 테스트해 봅시다:

```py
evaluate()
```

```python out
(10.934126853942871, 56057.14453125)
```

손실과 퍼플렉서티에 대한 이 값은 매우 높지만, 모델을 아직 학습하지 않았으므로 놀랍지는 않습니다. 이제 학습 스크립트의 핵심 부분인 학습 루프를 작성할 준비가 되었습니다. 학습 루프에서는 데이터 로더를 반복하고 배치를 모델에 전달합니다. 로짓을 사용하여 사용자 정의 손실 함수를 평가할 수 있습니다. 손실을 적절한 급격한 축소로 스케일링하여 더 많은 단계를 집계할 때 더 큰 손실을 만들지 않습니다. 최적화하기 전에 그라디언트를 클리핑하여 수렴을 개선합니다. 마지막으로, 일정한 간격으로 새로운 `evaluate()` 함수를 사용하여 평가 세트에서 모델을 평가합니다:

```py
from tqdm.notebook import tqdm

gradient_accumulation_steps = 8
eval_steps = 5_000

model.train()
completed_steps = 0
for epoch in range(num_train_epochs):
    for step, batch in tqdm(
        enumerate(train_dataloader, start=1), total=num_training_steps
    ):
        logits = model(batch["input_ids"]).logits
        loss = keytoken_weighted_loss(batch["input_ids"], logits, keytoken_ids)
        if step % 100 == 0:
            accelerator.print(
                {
                    "samples": step * samples_per_step,
                    "steps": completed_steps,
                    "loss/train": loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        if step % gradient_accumulation_steps == 0:
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += 1
        if (step % (eval_steps * gradient_accumulation_steps)) == 0:
            eval_loss, perplexity = evaluate()
            accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            if accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=f"Training in progress step {step}", blocking=False
                )
```

이것으로 모든 준비가 끝났습니다. 이제 GPT-2와 같은 인과 언어 모델에 대한 사용자 정의 학습 루프를 직접 작성하여 원하는 대로 사용자 정의할 수 있습니다.

<Tip>

✏️ **해보기!** 사용자 정의 손실 함수를 작성하여 사용 사례에 맞게 맞춤화하거나, 학습 루프에 다른 사용자 정의 단계를 추가해 보세요.

</Tip>

<Tip>

✏️ **해보기!** 긴 학습 실험을 실행할 때는 TensorBoard 또는 Weights & Biases와 같은 도구를 사용하여 중요한 메트릭을 로그하는 것이 좋습니다. 학습 루프에 적절한 로깅을 추가하여 항상 학습 상태를 확인할 수 있습니다.

</Tip>

{/if}
